# Curriculum Roadmap

> Mastery path from probability to machine learning, causal inference, and statistical mechanics.

---

## 1. Probability Foundations
**Goal:** formalize randomness and frequency.  
**Topics**
- random variables, pmf/pdf/cdf  
- independence, conditional probability  
- distributions: uniform → normal → exponential  
- law of large numbers, central limit theorem  
**Interactive:** Monte Carlo π, coin/die convergence.

---

## 2. Behavior of a Random Variable
**Goal:** understand shape, tails, and transformations.  
**Topics**
- transformations, mixtures, truncation  
- heavy tails, power laws  
- kernel density estimation, entropy  
**Interactive:** mixture slider, tail log–log plot.

---

## 3. Joint Distributions and Dependence
**Goal:** model relationships among variables.  
**Topics**
- covariance, correlation, conditional independence  
- multivariate normal, copulas  
- mutual information, graphical factorization  
**Interactive:** correlation heatmap, copula explorer.

---

## 4. Inference and Estimation
**Goal:** learn parameters from data.  
**Topics**
- sampling distributions, confidence intervals  
- MLE, likelihood ratio tests  
- Bayesian inference, conjugacy  
**Interactive:** bootstrap, likelihood surface demo.

---

## 5. Regression and Generalized Linear Models
**Goal:** model conditional expectations.  
**Topics**
- linear, logistic, Poisson regression  
- bias–variance tradeoff  
- regularization (ridge, lasso)  
**Interactive:** fit visualizer, residual explorer.

---

## 6. Bayesian Modeling and Hierarchies
**Goal:** encode uncertainty and partial pooling.  
**Topics**
- hierarchical priors, shrinkage  
- posterior predictive checks  
- model comparison (WAIC, LOO)  
**Interactive:** posterior update demo.

---

## 7. Causal Inference
**Goal:** infer effects of interventions.  
**Topics**
- DAGs, d-separation, backdoor/frontdoor criteria  
- potential outcomes, ATE, mediation  
- instrumental variables  
**Interactive:** editable DAG with do-calculus simulation.

---

## 8. Machine Learning Foundations
**Goal:** unify probability and optimization.  
**Topics**
- empirical risk minimization  
- gradient descent, regularization  
- generative vs discriminative models  
**Interactive:** loss-surface visualizer.

---

## 9. Advanced Machine Learning
**Goal:** handle complexity and representation.  
**Topics**
- ensembles, kernels, neural networks  
- latent variable models, VAEs, diffusion  
- reinforcement learning  
**Interactive:** stochastic gradient playground.

---

## 10. Statistical Mechanics Connections
**Goal:** link inference, learning, and physics.  
**Topics**
- Boltzmann distributions, entropy, free energy  
- partition function ↔ likelihood normalization  
- temperature ↔ regularization  
**Interactive:** energy landscape and entropy–energy tradeoff.

---

## 11. Unified Principles
**Goal:** synthesize information, energy, and learning.  
1. Learning = minimizing expected energy.  
2. Entropy quantifies ignorance.  
3. Bayesian update = information exchange.  
4. Gradient descent = relaxation to equilibrium.  
5. Regularization = temperature control.

---

**Next steps**
- Expand each module into its own markdown file under `/lessons/`.  
- Use the roadmap to drive navigation and progression indicators.
